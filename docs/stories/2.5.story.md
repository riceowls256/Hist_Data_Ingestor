# Story 2.5: Define and Implement Data Validation Rules for Decoded Databento Records

## Status: COMPLETED ✅

## Story

As a Developer, I want to define and implement validation rules. Since the databento-python client and the provided Pydantic models (from "Databento Downloader: Detailed Specifications" document or custom models for schemas like TBBO, Statistics) already perform significant schema validation and type conversion: Initial validation will rely on the successful instantiation of these Databento-specific Pydantic models. Post-transformation (to standardized internal model) checks (e.g., using Pandera or custom functions) will focus on business rules and consistency checks outlined in NFR 3 (e.g., positive prices, High >= Low for OHLCV), so that the integrity and quality of Databento data are ensured before storage.

## Acceptance Criteria (ACs)

1. **Leverage Databento Pydantic Models for Initial Validation**: DatabentoAdapter ensures DBN records convert to strict Pydantic models. Failed instantiations are logged & quarantined (NFR 3). ✅ COMPLETED

2. **Post-Transformation Databento Data Quality Checks Defined**: Validation rules (Pandera schemas or custom functions in src/transformation/validators/databento_validators.py) defined for standardized data from Databento for all schemas. Checks include positive numerics for OHLCV, High >= Low, valid timestamps, non-empty/expected symbol, consistency for TBBO and Statistics fields. ✅ COMPLETED

3. **Validation Logic Implemented & Integrated**: Post-transformation checks implemented and integrated into Databento data flow. ✅ COMPLETED

4. **Validation Failure Handling (Log & Quarantine Implemented)**: Failed post-transformation records are logged (record, rule) and quarantined (NFR 3). ✅ COMPLETED

5. **Unit Tests for Validation Rules**: Unit tests for Databento Pydantic models (validation/rejection) and custom post-transformation validation functions/schemas for all relevant schemas. ✅ COMPLETED

## Tasks / Subtasks

- [x] **Task 1: Enhance DatabentoAdapter Pydantic Validation** (AC: 1)
  - [x] Add strict validation mode to all Databento Pydantic model instantiations
  - [x] Implement try-catch blocks around model creation with detailed error logging
  - [x] Create quarantine mechanism for failed Pydantic instantiations
  - [x] Add validation failure metrics tracking

- [x] **Task 2: Create Databento-Specific Validation Rules** (AC: 2)
  - [x] Create `src/transformation/validators/databento_validators.py` with Pandera schemas
  - [x] Define OHLCV validation rules (positive prices, High >= Low >= Open, Close)
  - [x] Define Trade validation rules (positive price and size, valid side codes)
  - [x] Define TBBO validation rules (bid <= ask, positive prices and sizes)
  - [x] Define Statistics validation rules (valid stat_type codes, consistent data)
  - [x] Define Definition validation rules (expiration > activation, valid instrument_class)

- [x] **Task 3: Implement Custom Business Logic Validators** (AC: 2, 3)
  - [x] Create timestamp validation functions (timezone awareness, reasonable ranges)
  - [x] Create symbol validation functions (non-empty, expected format patterns)
  - [x] Create cross-field consistency validators for each schema type
  - [x] Implement validation severity levels (ERROR, WARNING, INFO)

- [x] **Task 4: Integrate Validation into Data Flow** (AC: 3)
  - [x] Update DataTransformationEngine to call post-transformation validators (implemented in RuleEngine)
  - [x] Add validation step in PipelineOrchestrator sequence (via RuleEngine integration)
  - [x] Ensure validation runs after transformation but before storage (implemented in RuleEngine)
  - [x] Add validation performance monitoring and timing (via structured logging)

- [x] **Task 5: Implement Quarantine and Logging System** (AC: 4)
  - [x] Create quarantine data storage structure (file-based or table-based)
  - [x] Implement detailed validation failure logging with structlog
  - [x] Create validation failure reports and metrics
  - [x] Add quarantine record retrieval and reprocessing capabilities

- [x] **Task 6: Create Comprehensive Unit Tests** (AC: 5)
  - [x] Test Pydantic model validation with valid/invalid data for all schemas
  - [x] Test business rule validators with edge cases and boundary conditions
  - [x] Test quarantine mechanism with various failure scenarios
  - [x] Test validation integration within the pipeline flow
  - [x] Create test fixtures for validation scenarios

## Dev Technical Guidance

### **Validation Architecture Pattern**
Follow the **two-stage validation pattern** established in the architecture:
1. **Stage 1**: Pydantic model validation (type safety, required fields)
2. **Stage 2**: Business logic validation (domain rules, cross-field consistency)

### **Key Technical Dependencies**

**Existing Models to Validate**: All models in `src/storage/models.py`:
- `DatabentoOHLCVRecord` (73 fields)
- `DatabentoTradeRecord` 
- `DatabentoTBBORecord`
- `DatabentoStatisticsRecord`  
- `DatabentoDefinitionRecord` (73 fields with complex leg structures)

**Validation Libraries**:
- **Pandera**: Use for DataFrame-style validation schemas
- **Pydantic**: Built-in validation for model instantiation
- **Custom Functions**: For complex business logic that Pandera cannot express

### **Critical Business Rules to Implement**

**OHLCV Validation Rules**:
```python
# Price consistency rules
high >= max(open, close, low)
low <= min(open, close, high)  
open, high, low, close > 0
volume >= 0
count >= 0 (if present)
```

**Trade Validation Rules**:
```python
# Trade data integrity
price > 0
size > 0
side in ['A', 'B', 'N'] or None
ts_event <= ts_recv (if both present)
```

**TBBO Validation Rules**:
```python
# Bid/Ask relationship
if bid_px and ask_px: bid_px <= ask_px
bid_sz >= 0, ask_sz >= 0 (if present)
bid_ct >= 0, ask_ct >= 0 (if present)
```

**Definition Validation Rules**:
```python
# Contract specification rules
expiration > activation
min_price_increment > 0
unit_of_measure_qty > 0
leg_count >= 0
if leg_count > 0: leg_index is not None
```

### **Quarantine Implementation Strategy**

**File-Based Quarantine Structure**:
```
dlq/
  validation_failures/
    {timestamp}/
      ohlcv_failures.json
      trade_failures.json  
      tbbo_failures.json
      statistics_failures.json
      definition_failures.json
```

**Quarantine Record Format**:
```python
{
  "timestamp": "2024-12-01T10:30:00Z",
  "schema_type": "ohlcv",
  "validation_rule": "price_consistency", 
  "error_message": "High price 100.50 < Low price 101.00",
  "original_record": { ... },
  "transformed_record": { ... }
}
```

### **Integration Points**

**DataTransformationEngine Enhancement**:
- Add validation step after field mapping
- Call `databento_validators.validate_{schema_type}()`
- Handle ValidationError exceptions

**PipelineOrchestrator Sequence**:
```
1. API Extraction → 2. Pydantic Validation → 3. Transformation → 
4. Business Validation → 5. Storage (or Quarantine)
```

### **Performance Considerations**

- **Batch Validation**: Validate records in batches using Pandera DataFrames
- **Lazy Validation**: Only validate fields that exist (handle Optional fields gracefully)
- **Metrics Collection**: Track validation pass/fail rates per schema type
- **Memory Management**: Stream large datasets through validation without loading all into memory

### **Configuration Integration**

Extend `configs/api_specific/databento_config.yaml`:
```yaml
validation:
  strict_mode: true
  quarantine_enabled: true
  validation_rules:
    ohlcv:
      price_tolerance: 0.01
      volume_max_limit: 1000000000
    trades:
      size_max_limit: 100000
```

### **Testing Strategy**

**Test Data Categories**:
1. **Valid Records**: Normal market data that should pass all validation
2. **Invalid Pydantic**: Data that fails model instantiation
3. **Invalid Business Logic**: Data that fails domain rules 
4. **Edge Cases**: Boundary conditions, null values, extreme values
5. **Performance Tests**: Large batches, memory usage validation

**Existing Test Infrastructure**:
- Leverage `tests/unit/core/test_definition_record_model.py` patterns
- Use fixtures from `tests/fixtures/` for sample data
- Follow test patterns in `tests/unit/transformation/`

## Story Progress Notes

### Agent Model Used: `BMad Scrum Master (Fran)`

### Completion Notes List

**Implementation Summary:**
Successfully implemented a comprehensive two-stage validation system for Databento data:

**Stage 1: Pydantic Validation (DatabentoAdapter)**
- Enhanced `DatabentoAdapter` with strict Pydantic validation and quarantine mechanism
- Created `QuarantineManager` utility for structured error handling
- Added validation statistics tracking and detailed error logging

**Stage 2: Pandera Business Logic Validation (RuleEngine)**
- Created comprehensive `databento_validators.py` with Pandera schemas for all data types
- Implemented custom business logic validators for timestamps, symbols, and cross-field consistency
- Added validation severity levels (ERROR, WARNING, INFO) for flexible error handling
- Integrated validation into `RuleEngine` with schema dispatcher pattern

**Key Features Delivered:**
1. **Complete Schema Coverage**: OHLCV, Trade, TBBO, Statistics, and Definition validation schemas
2. **Custom Business Logic**: Timezone validation, symbol format validation, cross-field consistency checks
3. **Quarantine System**: Failed records written to structured JSON files in `dlq/validation_failures/`
4. **Configuration Integration**: Extended `databento_config.yaml` with comprehensive validation settings
5. **Comprehensive Testing**: 24 unit tests covering all validation scenarios and edge cases
6. **Modern Pandera Usage**: Used Context7 to ensure current library patterns (DataFrameModel vs deprecated SchemaModel)

**Files Created/Modified:**
- `src/transformation/validators/databento_validators.py` (NEW - 300+ lines)
- `src/utils/file_io.py` (NEW - QuarantineManager)
- `src/ingestion/api_adapters/databento_adapter.py` (ENHANCED)
- `src/transformation/rule_engine/engine.py` (ENHANCED)
- `src/transformation/mapping_configs/databento_mappings.yaml` (ENHANCED)
- `configs/api_specific/databento_config.yaml` (ENHANCED)
- `tests/unit/transformation/test_databento_validators.py` (NEW - 400+ lines)

**Technical Achievements:**
- Two-stage validation architecture providing both type safety and business logic validation
- Graceful error handling with configurable severity levels
- Performance-optimized batch validation using Pandera DataFrames
- Schema dispatcher pattern for dynamic validation selection
- Comprehensive business rules covering all CME Globex data requirements

### Change Log

- **2024-12-01**: Story created with comprehensive technical guidance and task breakdown
- **2024-12-14**: Story completed with full implementation of two-stage validation system 