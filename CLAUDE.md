# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

The Historical Data Ingestor is a production-ready Python system for ingesting, processing, and querying historical financial market data. It uses a modular monolith architecture with a plugin-based design for API providers (currently Databento).

## Common Development Commands

### Important Note on Date Ranges
When using the `ingest` command, the start date and end date **MUST** be different. The Databento API returns an error if start_date equals end_date. Always use at least a 1-day range (e.g., 2024-01-01 to 2024-01-02).

### Running the Application

```bash
# Basic CLI usage
python main.py --help
python main.py status
python main.py version

# Data ingestion
python main.py ingest --api databento --job ohlcv_1d
python main.py ingest --api databento --dataset GLBX.MDP3 --schema ohlcv-1d --symbols CL.FUT,ES.FUT --start-date 2023-01-01 --end-date 2023-12-31

# Definition schema ingestion (production-ready as of 2025-06-19)
python main.py ingest --api databento --dataset GLBX.MDP3 --schema definitions --symbols ES.FUT,CL.FUT --stype-in parent --start-date 2024-04-15 --end-date 2024-05-05

# Data querying
python main.py query -s ES.c.0 --start-date 2024-01-01 --end-date 2024-01-31
python main.py query --symbols ES.c.0,NQ.c.0 --start-date 2024-01-01 --end-date 2024-01-31 --output-format csv
```

### Testing

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/unit/test_databento_adapter.py

# Run with coverage
pytest --cov=src --cov-report=html

# Run E2E tests
source .env.test
python tests/integration/test_databento_e2e_pipeline.py

# MVP verification
python run_mvp_verification.py
```

### Linting and Type Checking

```bash
# Format code with Black
black src/ tests/

# Lint with Ruff
ruff check src/ tests/

# Type check with MyPy
mypy src/
```

### Docker Operations

```bash
# Start services
docker-compose up --build -d

# Stop services
docker-compose down

# View logs
docker-compose logs -f

# Execute commands in container
docker-compose exec app python main.py status
```

## High-Level Architecture

The system follows a modular monolith pattern with these key components:

### Core Pipeline Flow
```
CLI â†’ PipelineOrchestrator â†’ APIAdapter â†’ RuleEngine â†’ Validator â†’ StorageLoader â†’ TimescaleDB
```

### Key Design Patterns

1. **Adapter Pattern**: Each API provider implements `BaseAdapter` for API-agnostic design
2. **Pipeline Pattern**: `PipelineOrchestrator` manages the ETL flow with centralized error handling
3. **Rule Engine Pattern**: YAML-based declarative transformations via `RuleEngine`
4. **Repository Pattern**: Storage abstraction through `TimescaleLoader` classes

### Component Responsibilities

- **src/cli/**: Typer-based CLI interface
- **src/core/**: Framework components (config, pipeline orchestration)
- **src/ingestion/**: API adapters and data fetching
- **src/transformation/**: Data transformation rules and validation
- **src/storage/**: TimescaleDB interaction layer
- **src/querying/**: Query building with intelligent symbol resolution
- **src/utils/**: Logging and utilities

### Important Architectural Decisions

1. **TimescaleDB**: Optimized hypertables for time-series data with automatic partitioning
2. **Schema-Based Routing**: Different storage strategies for OHLCV vs Definitions data
3. **Intelligent Symbol Resolution**: Automatic mapping between symbols and instrument IDs
4. **Structured Logging**: JSON-formatted logs with contextual information using structlog
5. **Configuration Hierarchy**: YAML configs for system, API-specific, and transformation rules

### Error Handling Strategy

- Retry logic with exponential backoff for transient failures
- Dead Letter Queue (DLQ) for persistent failures at `dlq/`
- Transaction-based writes with rollback capability
- Graceful degradation with quarantine for problematic records

## Important Context

- The project has reached MVP status with 98.7% test coverage
- Currently focused on Databento API integration (databento_only branch)
- **Definitions schema is production-ready** (2025-06-19) with excellent performance
- Uses Pydantic for data validation throughout the pipeline
- Follows PEP 8 standards and uses Black/Ruff for code formatting
- Docker-ready with containerized deployment
- Environment variables manage sensitive configuration (API keys, passwords)
- BMad Method orchestrator configuration available for AI-assisted development

## Recent Changes & Updates

### ðŸŽ‰ Definitions Schema Production Success (2025-06-19)
**MAJOR BREAKTHROUGH**: Successfully transformed definitions schema from completely non-functional to production-ready.

**Achievement: Production-Ready Performance**
- **33,829 records processed in 9.16 seconds** (3,693 records/second)
- **Zero validation errors** across entire pipeline
- **All 73+ definition fields** properly mapped and validated
- **Complete end-to-end pipeline** working (API â†’ Transform â†’ Validate â†’ Storage)

**Key Technical Implementations:**
- **Field Mapping Resolution**: Added comprehensive field name mapping in both `databento_adapter.py` and `pipeline_orchestrator.py` to handle API field names â†’ Pydantic model field names
- **Schema Normalization**: "definitions" â†’ "definition" alias working perfectly end-to-end
- **Pipeline Architecture**: Complete validation and repair logic working consistently
- **Performance Optimization**: Excellent processing speed with comprehensive error handling

**Production Status**: The definitions schema is now **production-ready** and can be used for live data ingestion with excellent performance.

**Remaining Polish Items**: 
- NUL character cleaning in storage layer (medium priority)
- CLI validation for ALL_SYMBOLS (low priority)
- Date range optimization documentation (low priority)

### CLI Refactoring Success (2025-06-18)
Successfully refactored the monolithic main.py file using a simple, pragmatic approach:

**Achievement: 99.4% Code Reduction**
- Before: `src/main.py` - 2,509 lines  
- After: `src/main.py` - 16 lines
- Implementation: Renamed original to `cli_commands.py`, created clean entry point

**Key Lessons:**
- Simple solutions (5 minutes) often beat complex approaches (3+ hours)
- Preserve working functionality whenever possible  
- File operations are safer than code restructuring
- See `docs/REFACTORING_LESSONS_LEARNED.md` for detailed analysis

**Result:** All 23 CLI commands work identically with a clean, maintainable entry point.

### CLI Job Naming (2025-06-17)
When using custom ingestion commands (without predefined job names), the system now automatically generates job names based on schema and symbols:
- Pattern: `cli_{schema}_{symbols}`
- Example: `cli_ohlcv-1d_ES.c.0_CL.c.0`
- This prevents "missing name field" validation errors

### Statistics Data Ingestion Fix (2025-06-17)
Fixed critical issue where statistics records were failing with "Unknown record type, cannot store":
- Enhanced databento_adapter.py to properly map API `price` field to model `stat_value` field
- Fixed statistics loader ON CONFLICT constraint to match table schema
- Added comprehensive field mapping for TBBO records (bid_px_00 â†’ bid_px, etc.)
- Enhanced TradeRecord model with missing fields and automatic quantity mapping
- Statistics ingestion now works correctly end-to-end

### Table Creation Handling (2025-06-17)
The TimescaleOHLCVLoader now gracefully handles existing tables:
- Uses `CREATE TABLE IF NOT EXISTS` for all table creation
- Logs informational messages instead of errors when tables already exist
- Continues execution without interruption

### Symbol Field Mapping Fix (2025-06-17)
Implemented robust symbol field validation and repair system to eliminate Pydantic validation failures:

**Problem Resolved:**
- Fixed critical issue where trade records were failing with "Field required [type=missing]" errors for symbol field
- Eliminated 315,493 validation errors, achieving 100% successful trade record ingestion (3,459/3,459 records stored)

**Technical Implementation:**
- Added `_ensure_symbol_field` method in `databento_adapter.py` for proactive symbol field mapping and validation
- Added `_validate_and_repair_record_dict` method in `pipeline_orchestrator.py` for dict-to-model conversion validation and repair
- Enhanced error handling with meaningful repair statistics logging
- Applied validation logic to both trades and TBBO schemas

**Key Features:**
- Automatic symbol field mapping from Databento API response fields
- Fallback logic using instrument_id and raw_symbol when symbol field is missing
- Comprehensive logging of validation failures and successful repairs
- Zero performance impact (processing remains at 2-3 seconds for thousands of records)

**Success Metrics:**
- âœ… 0 "Field required [type=missing]" errors for symbol field (down from 315,493)
- âœ… 100% trade record ingestion success rate
- âœ… All ingested records have valid symbol field values
- âœ… Fast processing with comprehensive monitoring

### Field Standardization (2024-06-16)

### Critical: Field Naming Convention
The codebase underwent comprehensive field standardization to resolve validation issues:

**OHLCV Fields:**
- `open` â†’ `open_price`
- `high` â†’ `high_price`
- `low` â†’ `low_price`
- `close` â†’ `close_price`
- `count` â†’ `trade_count`

**TBBO Fields:**
- `bid_px_00` â†’ `bid_px`
- `ask_px_00` â†’ `ask_px`
- `bid_sz_00` â†’ `bid_sz`
- `ask_sz_00` â†’ `ask_sz`

### Validation Schema Updates
- All Pandera schemas now use `strict = False` to allow extra fields
- Statistics schema accepts both `stat_value` and `price` fields for compatibility
- Type coercion enabled (`coerce = True`) for flexible type handling

### Affected Components
When working with these fields, be aware of updates in:
- `src/storage/models.py` - Pydantic models
- `src/ingestion/api_adapters/databento_adapter.py` - Field mapping logic
- `src/transformation/validators/databento_validators.py` - Validation schemas
- All test fixtures and test files

### Documentation
For detailed information about this migration:
- See `docs/FIELD_STANDARDIZATION_MIGRATION.md` for migration details
- See `docs/DEBUGGING_LESSONS_LEARNED.md` for debugging strategies

### Testing Status
As of 2024-06-16:
- All 152 unit tests passing
- All transformation tests passing
- Field validation working correctly across all schemas

### Pandera DataFrame Dtype Coercion Fix (2025-06-18)
Fixed critical Pandera validation error that was blocking OHLCV data ingestion:

**Problem Resolved:**
- Fixed `coerce_dtype('int64')` failures for `trade_count` column in OHLCV data
- Error occurred when pandas inferred `float64` dtype for mixed integer/None values, then Pandera failed to coerce to `pd.Int64Dtype`

**Root Cause:**
When `pd.DataFrame(transformed_batch)` creates DataFrames with mixed integer and `None` values for `trade_count`, pandas automatically infers `float64` dtype (converting `None` to `NaN`). Later, when Pandera tries to validate and coerce this to `pd.Int64Dtype`, it fails because pandas cannot directly coerce `float64` with `NaN` to regular `int64`.

**Technical Solution:**
Enhanced `src/transformation/rule_engine/engine.py` to explicitly convert nullable integer columns to proper pandas extension dtypes before Pandera validation:

```python
# Fix nullable integer columns that pandas infers as float64
# This prevents Pandera coercion errors when validating nullable int columns
if 'trade_count' in df.columns:
    df['trade_count'] = df['trade_count'].astype('Int64')
```

**Impact:**
- âœ… OHLCV ingestion now processes without validation errors
- âœ… All 506 records processed successfully in test runs
- âœ… Maintains existing functionality while fixing the validation pipeline

### Schema Validation Consistency Enhancement (2025-06-18)
Standardized validation and repair logic across all data schemas:

**Problem Identified:**
- OHLCV and Statistics schemas lacked the validation/repair system that Trades and TBBO schemas already had
- Inconsistent error handling and missing symbol field repair for OHLCV data

**Solution Implemented:**
Updated `src/core/pipeline_orchestrator.py` to apply consistent validation and repair logic across all schemas:

```python
# Pre-validate and repair if needed
validated_dict = self._validate_and_repair_record_dict(record_dict, schema, job_config)

if validated_dict is None:
    repair_stats['failed_repair'] += 1
    self.stats.errors_encountered += 1
    continue

if validated_dict != record_dict:
    repair_stats['repaired'] += 1
```

**Schemas Enhanced:**
- âœ… OHLCV schema now includes validation and repair logic
- âœ… Statistics schema updated with consistent error handling
- âœ… All schemas now log repair statistics for monitoring
- âœ… Symbol field repair works consistently across all data types

**Testing Validation:**
- âœ… All 30 transformation tests passing
- âœ… All 51 core functionality tests passing
- âœ… Updated trade schema test to reflect negative price support for spread instruments
- âœ… Updated job config test to expect automatic name generation

**Files Modified:**
- `src/transformation/rule_engine/engine.py` - DataFrame dtype conversion fix
- `src/core/pipeline_orchestrator.py` - Consistent validation/repair logic
- `tests/unit/transformation/test_databento_validators.py` - Updated negative price test
- `tests/unit/core/test_pipeline_orchestrator.py` - Updated job config test